{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Цепочки и последовательности промптов\n",
        "\n",
        "## Обзор\n",
        "\n",
        "В этом уроке разбираются цепочки промптов (prompt chaining) и последовательное промптирование (sequencing) — приёмы, при которых выход одного промпта становится входом следующего.\n",
        "\n",
        "## Мотивация\n",
        "\n",
        "Чем сложнее задача, тем чаще её нужно разбивать на шаги. Цепочки промптов позволяют провести модель через серию связанных запросов — это даёт более структурированный и управляемый результат. Подход особенно полезен, когда задача требует нескольких этапов обработки или принятия решений.\n",
        "\n",
        "## Ключевые компоненты\n",
        "\n",
        "1. **Базовая цепочка промптов**: выход одного промпта подаётся на вход другого.\n",
        "2. **Последовательное промптирование**: логическая последовательность промптов для многошагового анализа.\n",
        "3. **Динамическая генерация промптов**: следующий промпт формируется на основе ответа модели.\n",
        "4. **Обработка ошибок и валидация**: проверки внутри цепочки для повышения надёжности."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Установка окружения"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import re\n",
        "\n",
        "from dotenv import load_dotenv\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain.prompts import PromptTemplate\n",
        "\n",
        "from dotenv import load_dotenv\n",
        "load_dotenv(os.path.join(os.path.dirname(os.getcwd()), \".env\"))\n",
        "load_dotenv()\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = os.getenv('OPENAI_API_KEY')\n",
        "base_url = os.getenv('BASE_URL')\n",
        "\n",
        "llm = ChatOpenAI(model=\"gpt-4o-mini\", base_url=base_url)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Базовая цепочка промптов\n",
        "\n",
        "Начнём с простого примера: первый промпт генерирует объяснение учебной темы для школьников, второй — сжимает его в одно ключевое предложение."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Шаблоны промптов\n",
        "explain_prompt = PromptTemplate(\n",
        "    input_variables=[\"topic\"],\n",
        "    template=\"Объясни тему «{topic}» простым языком для школьника 7-го класса. 3–4 предложения.\"\n",
        ")\n",
        "\n",
        "summarize_prompt = PromptTemplate(\n",
        "    input_variables=[\"explanation\"],\n",
        "    template=\"Сократи следующее объяснение до одного ключевого предложения:\\n{explanation}\"\n",
        ")\n",
        "\n",
        "# Цепочка промптов\n",
        "def explain_and_summarize(topic):\n",
        "    \"\"\"Генерирует объяснение темы и сжимает его в одно предложение.\n",
        "\n",
        "    Args:\n",
        "        topic (str): Учебная тема.\n",
        "\n",
        "    Returns:\n",
        "        tuple: (объяснение, краткий итог).\n",
        "    \"\"\"\n",
        "    explanation = (explain_prompt | llm).invoke({\"topic\": topic}).content\n",
        "    summary = (summarize_prompt | llm).invoke({\"explanation\": explanation}).content\n",
        "    return explanation, summary\n",
        "\n",
        "# Тестируем цепочку\n",
        "topic = \"фотосинтез\"\n",
        "explanation, summary = explain_and_summarize(topic)\n",
        "print(f\"Объяснение:\\n{explanation}\\n\\nКлючевое предложение:\\n{summary}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Последовательное промптирование\n",
        "\n",
        "Создадим более сложную последовательность промптов для многошагового анализа. Возьмём текст о методике преподавания и определим его основную тему, тон и ключевые выводы."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Шаблоны промптов для каждого шага анализа\n",
        "theme_prompt = PromptTemplate(\n",
        "    input_variables=[\"text\"],\n",
        "    template=\"Определи основную тему следующего текста:\\n{text}\"\n",
        ")\n",
        "\n",
        "tone_prompt = PromptTemplate(\n",
        "    input_variables=[\"text\"],\n",
        "    template=\"Опиши общий тон следующего текста:\\n{text}\"\n",
        ")\n",
        "\n",
        "takeaway_prompt = PromptTemplate(\n",
        "    input_variables=[\"text\", \"theme\", \"tone\"],\n",
        "    template=\"Учитывая текст с основной темой «{theme}» и тоном «{tone}», сформулируй ключевые выводы:\\n{text}\"\n",
        ")\n",
        "\n",
        "def analyze_text(text):\n",
        "    \"\"\"Многошаговый анализ текста: тема, тон, выводы.\n",
        "\n",
        "    Args:\n",
        "        text (str): Текст для анализа.\n",
        "\n",
        "    Returns:\n",
        "        dict: Словарь с темой, тоном и ключевыми выводами.\n",
        "    \"\"\"\n",
        "    theme = (theme_prompt | llm).invoke({\"text\": text}).content\n",
        "    tone = (tone_prompt | llm).invoke({\"text\": text}).content\n",
        "    takeaways = (takeaway_prompt | llm).invoke({\"text\": text, \"theme\": theme, \"tone\": tone}).content\n",
        "    return {\"Тема\": theme, \"Тон\": tone, \"Выводы\": takeaways}\n",
        "\n",
        "# Тестируем последовательное промптирование\n",
        "sample_text = \"\"\"Современные исследования в области педагогики показывают, что активное обучение значительно \n",
        "эффективнее пассивного прослушивания лекций. Когда ученики решают задачи, обсуждают материал в группах \n",
        "и применяют знания на практике, уровень усвоения возрастает в разы. Однако переход к активным методам \n",
        "требует от учителя перестройки подхода: нужно больше времени на подготовку, умение управлять групповой \n",
        "динамикой и готовность отойти от привычного формата «учитель говорит — ученик слушает». Несмотря на \n",
        "сложности, школы, внедрившие активное обучение, фиксируют рост мотивации и успеваемости учащихся.\"\"\"\n",
        "\n",
        "analysis = analyze_text(sample_text)\n",
        "for key, value in analysis.items():\n",
        "    print(f\"{key}: {value}\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Динамическая генерация промптов\n",
        "\n",
        "Создадим систему «вопрос — ответ», которая автоматически генерирует уточняющие вопросы на основе предыдущих ответов."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Шаблоны промптов\n",
        "answer_prompt = PromptTemplate(\n",
        "    input_variables=[\"question\"],\n",
        "    template=\"Ответь на вопрос кратко:\\n{question}\"\n",
        ")\n",
        "\n",
        "follow_up_prompt = PromptTemplate(\n",
        "    input_variables=[\"question\", \"answer\"],\n",
        "    template=\"На основе вопроса «{question}» и ответа «{answer}» сформулируй уточняющий вопрос по теме.\"\n",
        ")\n",
        "\n",
        "def dynamic_qa(initial_question, num_follow_ups=3):\n",
        "    \"\"\"Динамическая сессия «вопрос — ответ» с автоматическими уточняющими вопросами.\n",
        "\n",
        "    Args:\n",
        "        initial_question (str): Начальный вопрос.\n",
        "        num_follow_ups (int): Количество уточняющих вопросов.\n",
        "\n",
        "    Returns:\n",
        "        list: Список словарей с вопросами и ответами.\n",
        "    \"\"\"\n",
        "    qa_chain = []\n",
        "    current_question = initial_question\n",
        "\n",
        "    for i in range(num_follow_ups + 1):  # +1 для начального вопроса\n",
        "        answer = (answer_prompt | llm).invoke({\"question\": current_question}).content\n",
        "        qa_chain.append({\"question\": current_question, \"answer\": answer})\n",
        "        \n",
        "        if i < num_follow_ups:  # уточняющий вопрос для всех итераций, кроме последней\n",
        "            current_question = (follow_up_prompt | llm).invoke({\"question\": current_question, \"answer\": answer}).content\n",
        "\n",
        "    return qa_chain\n",
        "\n",
        "# Тестируем динамическую генерацию\n",
        "initial_question = \"Какие методы помогают повысить мотивацию учеников к учёбе?\"\n",
        "qa_session = dynamic_qa(initial_question)\n",
        "\n",
        "for i, qa in enumerate(qa_session):\n",
        "    print(f\"В{i+1}: {qa['question']}\")\n",
        "    print(f\"О{i+1}: {qa['answer']}\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Обработка ошибок и валидация\n",
        "\n",
        "В этой части добавляем обработку ошибок и проверку результатов внутри цепочки промптов, чтобы сделать её надёжнее."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Шаблоны промптов\n",
        "generate_prompt = PromptTemplate(\n",
        "    input_variables=[\"topic\"],\n",
        "    template=\"Назови 4-значное число, связанное с темой: {topic}. Ответь ТОЛЬКО числом, без пояснений.\"\n",
        ")\n",
        "\n",
        "validate_prompt = PromptTemplate(\n",
        "    input_variables=[\"number\", \"topic\"],\n",
        "    template=\"Число {number} действительно связано с темой «{topic}»? Ответь «Да» или «Нет» и объясни почему.\"\n",
        ")\n",
        "\n",
        "def extract_number(text):\n",
        "    \"\"\"Извлекает 4-значное число из текста.\n",
        "\n",
        "    Args:\n",
        "        text (str): Текст для извлечения числа.\n",
        "\n",
        "    Returns:\n",
        "        str or None: 4-значное число или None, если не найдено.\n",
        "    \"\"\"\n",
        "    match = re.search(r'\\b\\d{4}\\b', text)\n",
        "    return match.group() if match else None\n",
        "\n",
        "def robust_number_generation(topic, max_attempts=3):\n",
        "    \"\"\"Генерация числа, связанного с темой, с валидацией и обработкой ошибок.\n",
        "\n",
        "    Args:\n",
        "        topic (str): Тема для генерации числа.\n",
        "        max_attempts (int): Максимальное количество попыток.\n",
        "\n",
        "    Returns:\n",
        "        str: Валидированное 4-значное число или сообщение об ошибке.\n",
        "    \"\"\"\n",
        "    for attempt in range(max_attempts):\n",
        "        try:\n",
        "            response = (generate_prompt | llm).invoke({\"topic\": topic}).content\n",
        "            number = extract_number(response)\n",
        "            \n",
        "            if not number:\n",
        "                raise ValueError(f\"Не удалось извлечь 4-значное число из ответа: {response}\")\n",
        "            \n",
        "            # Валидация связи числа с темой\n",
        "            validation = (validate_prompt | llm).invoke({\"number\": number, \"topic\": topic}).content\n",
        "            if validation.lower().startswith(\"да\"):\n",
        "                return number\n",
        "            else:\n",
        "                print(f\"Попытка {attempt + 1}: число {number} не прошло валидацию. Причина: {validation}\")\n",
        "        except Exception as e:\n",
        "            print(f\"Попытка {attempt + 1} не удалась: {str(e)}\")\n",
        "    \n",
        "    return \"Не удалось сгенерировать валидное число после нескольких попыток.\"\n",
        "\n",
        "# Тестируем генерацию с валидацией\n",
        "topic = \"Великая Отечественная война\"\n",
        "result = robust_number_generation(topic)\n",
        "print(f\"Результат для темы «{topic}»: {result}\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
