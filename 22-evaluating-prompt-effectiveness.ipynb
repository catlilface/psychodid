{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Оценка эффективности промптов\n",
        "\n",
        "## Обзор\n",
        "В этом уроке рассматриваются методы и техники оценки эффективности промптов для языковых моделей. Разбираются метрики качества, а также ручная и автоматизированная оценка.\n",
        "\n",
        "## Мотивация\n",
        "Чтобы улучшать промпты, нужно уметь измерять их качество. Систематическая оценка позволяет находить наиболее эффективные формулировки и получать более надёжные ответы от модели.\n",
        "\n",
        "## Ключевые компоненты\n",
        "1. Метрики качества промптов.\n",
        "2. Ручная оценка.\n",
        "3. Автоматизированная оценка.\n",
        "4. Практические примеры с OpenAI и LangChain."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Установка окружения"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "from langchain_openai import ChatOpenAI\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import numpy as np\n",
        "\n",
        "from dotenv import load_dotenv\n",
        "load_dotenv(os.path.join(os.path.dirname(os.getcwd()), \".env\"))\n",
        "load_dotenv()\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = os.getenv('OPENAI_API_KEY')\n",
        "base_url = os.getenv('BASE_URL')\n",
        "\n",
        "llm = ChatOpenAI(model=\"gpt-4o-mini\", base_url=base_url)\n",
        "\n",
        "# Модель для вычисления семантического сходства\n",
        "sentence_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "\n",
        "def semantic_similarity(text1, text2):\n",
        "    \"\"\"Вычисление семантического сходства двух текстов через косинусное расстояние.\"\"\"\n",
        "    embeddings = sentence_model.encode([text1, text2])\n",
        "    return cosine_similarity([embeddings[0]], [embeddings[1]])[0][0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Метрики качества промптов\n",
        "\n",
        "Определим ключевые метрики для оценки эффективности промптов:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def relevance_score(response, expected_content):\n",
        "    \"\"\"Оценка релевантности: семантическое сходство ответа с ожидаемым содержанием.\"\"\"\n",
        "    return semantic_similarity(response, expected_content)\n",
        "\n",
        "def consistency_score(responses):\n",
        "    \"\"\"Оценка согласованности: сходство между несколькими ответами на один промпт.\"\"\"\n",
        "    if len(responses) < 2:\n",
        "        return 1.0  # Идеальная согласованность при одном ответе\n",
        "    similarities = []\n",
        "    for i in range(len(responses)):\n",
        "        for j in range(i+1, len(responses)):\n",
        "            similarities.append(semantic_similarity(responses[i], responses[j]))\n",
        "    return np.mean(similarities)\n",
        "\n",
        "def specificity_score(response):\n",
        "    \"\"\"Оценка специфичности: доля уникальных слов в ответе.\"\"\"\n",
        "    words = response.split()\n",
        "    unique_words = set(words)\n",
        "    return len(unique_words) / len(words) if words else 0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Ручная оценка\n",
        "\n",
        "Ручная оценка предполагает, что человек анализирует пару «промпт — ответ». Создадим функцию для имитации этого процесса:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def manual_evaluation(prompt, response, criteria):\n",
        "    \"\"\"Имитация ручной оценки пары «промпт — ответ».\"\"\"\n",
        "    print(f\"Промпт: {prompt}\")\n",
        "    print(f\"Ответ: {response}\")\n",
        "    print(\"\\nКритерии оценки:\")\n",
        "    for criterion in criteria:\n",
        "        score = float(input(f\"Оценка по критерию «{criterion}» (0-10): \"))\n",
        "        print(f\"{criterion}: {score}/10\")\n",
        "    print(\"\\nДополнительные комментарии:\")\n",
        "    comments = input(\"Введите комментарий: \")\n",
        "    print(f\"Комментарий: {comments}\")\n",
        "\n",
        "# Пример использования\n",
        "prompt = \"Объясни простыми словами, что такое дифференцированное обучение.\"\n",
        "response = llm.invoke(prompt).content\n",
        "criteria = [\"Ясность\", \"Точность\", \"Простота\"]\n",
        "manual_evaluation(prompt, response, criteria)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Автоматизированная оценка\n",
        "\n",
        "Реализуем автоматические методы оценки промптов:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def automated_evaluation(prompt, response, expected_content):\n",
        "    \"\"\"Автоматическая оценка пары «промпт — ответ».\"\"\"\n",
        "    relevance = relevance_score(response, expected_content)\n",
        "    specificity = specificity_score(response)\n",
        "    \n",
        "    print(f\"Промпт: {prompt}\")\n",
        "    print(f\"Ответ: {response}\")\n",
        "    print(f\"\\nРелевантность: {relevance:.2f}\")\n",
        "    print(f\"Специфичность: {specificity:.2f}\")\n",
        "    \n",
        "    return {\"relevance\": relevance, \"specificity\": specificity}\n",
        "\n",
        "# Пример использования\n",
        "prompt = \"Какие три основных метода оценки знаний учащихся существуют?\"\n",
        "expected_content = \"Три основных метода оценки знаний: формативная (текущая), суммативная (итоговая) и диагностическая оценка.\"\n",
        "response = llm.invoke(prompt).content\n",
        "automated_evaluation(prompt, response, expected_content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Сравнительный анализ\n",
        "\n",
        "Сравним эффективность разных промптов для одной и той же задачи:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def compare_prompts(prompts, expected_content):\n",
        "    \"\"\"Сравнение эффективности нескольких промптов для одной задачи.\"\"\"\n",
        "    results = []\n",
        "    for prompt in prompts:\n",
        "        response = llm.invoke(prompt).content\n",
        "        evaluation = automated_evaluation(prompt, response, expected_content)\n",
        "        results.append({\"prompt\": prompt, **evaluation})\n",
        "    \n",
        "    # Сортировка по релевантности\n",
        "    sorted_results = sorted(results, key=lambda x: x['relevance'], reverse=True)\n",
        "    \n",
        "    print(\"Результаты сравнения промптов:\")\n",
        "    for i, result in enumerate(sorted_results, 1):\n",
        "        print(f\"\\n{i}. Промпт: {result['prompt']}\")\n",
        "        print(f\"   Релевантность: {result['relevance']:.2f}\")\n",
        "        print(f\"   Специфичность: {result['specificity']:.2f}\")\n",
        "    \n",
        "    return sorted_results\n",
        "\n",
        "# Пример использования\n",
        "prompts = [\n",
        "    \"Перечисли методы оценки знаний учащихся.\",\n",
        "    \"Какие основные виды оценивания используются в школе?\",\n",
        "    \"Объясни разные подходы к оценке успеваемости.\"\n",
        "]\n",
        "expected_content = \"Основные методы оценки знаний: формативная (текущая), суммативная (итоговая) и диагностическая оценка.\"\n",
        "compare_prompts(prompts, expected_content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Комплексная оценка\n",
        "\n",
        "Создадим функцию, которая объединяет ручную и автоматическую оценку:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def evaluate_prompt(prompt, expected_content, manual_criteria=[\"Ясность\", \"Точность\", \"Релевантность\"]):\n",
        "    \"\"\"Комплексная оценка промпта: автоматическая + ручная.\"\"\"\n",
        "    response = llm.invoke(prompt).content\n",
        "    \n",
        "    print(\"Автоматическая оценка:\")\n",
        "    auto_results = automated_evaluation(prompt, response, expected_content)\n",
        "    \n",
        "    print(\"\\nРучная оценка:\")\n",
        "    manual_evaluation(prompt, response, manual_criteria)\n",
        "    \n",
        "    return {\"prompt\": prompt, \"response\": response, **auto_results}\n",
        "\n",
        "# Пример использования\n",
        "prompt = \"Объясни, что такое формативное оценивание и чем оно отличается от суммативного.\"\n",
        "expected_content = \"Формативное оценивание — это текущая проверка понимания в процессе обучения, направленная на обратную связь и корректировку, в отличие от суммативного оценивания, которое подводит итог за период.\"\n",
        "evaluate_prompt(prompt, expected_content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Итеративное улучшение через обратную связь\n",
        "\n",
        "Оценка ответа полезна сама по себе, но настоящая сила раскрывается, когда мы **замыкаем цикл**: передаём результат оценки обратно в модель и просим её улучшить ответ. Это позволяет автоматически повышать качество без ручной переработки промпта.\n",
        "\n",
        "Цикл работает так:\n",
        "1. Получаем ответ модели на промпт.\n",
        "2. Автоматически оцениваем его (релевантность, специфичность).\n",
        "3. Формируем промпт-обратную связь: исходный ответ + оценки + инструкция «улучши».\n",
        "4. Получаем улучшенный ответ и оцениваем снова.\n",
        "5. Сравниваем метрики до и после."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def iterative_improve(prompt, expected_content, max_iterations=3, target_relevance=0.85):\n",
        "    \"\"\"Итеративное улучшение ответа: оценка → обратная связь → повторная генерация.\"\"\"\n",
        "    \n",
        "    response = llm.invoke(prompt).content\n",
        "    relevance = relevance_score(response, expected_content)\n",
        "    specificity = specificity_score(response)\n",
        "    \n",
        "    print(f\"--- Итерация 0 (исходный ответ) ---\")\n",
        "    print(f\"Релевантность: {relevance:.2f} | Специфичность: {specificity:.2f}\")\n",
        "    print(f\"Ответ: {response[:300]}...\")\n",
        "    print()\n",
        "    \n",
        "    history = [{\"iteration\": 0, \"relevance\": float(relevance), \"specificity\": float(specificity)}]\n",
        "    \n",
        "    for i in range(1, max_iterations + 1):\n",
        "        if relevance >= target_relevance:\n",
        "            print(f\"Целевая релевантность ({target_relevance}) достигнута. Остановка.\")\n",
        "            break\n",
        "        \n",
        "        # Формируем промпт с обратной связью\n",
        "        feedback_prompt = f\"\"\"Ты — помощник учителя. Тебе был задан вопрос, и ты дал ответ.\n",
        "Оцени результат и улучши его.\n",
        "\n",
        "Исходный вопрос: {prompt}\n",
        "\n",
        "Твой предыдущий ответ:\n",
        "{response}\n",
        "\n",
        "Автоматическая оценка предыдущего ответа:\n",
        "- Релевантность (семантическое сходство с эталоном): {relevance:.2f} из 1.00\n",
        "- Специфичность (доля уникальных слов): {specificity:.2f} из 1.00\n",
        "\n",
        "Что нужно улучшить:\n",
        "- {\"Повысь релевантность: ответ должен точнее соответствовать теме вопроса.\" if relevance < target_relevance else \"Релевантность в норме.\"}\n",
        "- {\"Повысь специфичность: избегай повторов, используй более точные формулировки.\" if specificity < 0.6 else \"Специфичность в норме.\"}\n",
        "\n",
        "Дай улучшенный ответ на исходный вопрос:\"\"\"\n",
        "        \n",
        "        response = llm.invoke(feedback_prompt).content\n",
        "        relevance = relevance_score(response, expected_content)\n",
        "        specificity = specificity_score(response)\n",
        "        \n",
        "        print(f\"--- Итерация {i} ---\")\n",
        "        print(f\"Релевантность: {relevance:.2f} | Специфичность: {specificity:.2f}\")\n",
        "        print(f\"Ответ: {response[:300]}...\")\n",
        "        print()\n",
        "        \n",
        "        history.append({\"iteration\": i, \"relevance\": float(relevance), \"specificity\": float(specificity)})\n",
        "    \n",
        "    # Итоговое сравнение\n",
        "    print(\"=\" * 50)\n",
        "    print(\"Сводка по итерациям:\")\n",
        "    for h in history:\n",
        "        delta_r = h[\"relevance\"] - history[0][\"relevance\"]\n",
        "        sign = \"+\" if delta_r >= 0 else \"\"\n",
        "        print(f\"  Итерация {h['iteration']}: релевантность={h['relevance']:.2f} ({sign}{delta_r:.2f}), специфичность={h['specificity']:.2f}\")\n",
        "    \n",
        "    return history"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Пример: улучшение ответа о методах оценки знаний\n",
        "\n",
        "Зададим вопрос об оценке знаний, запустим цикл улучшения и посмотрим, как меняются метрики."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Пример 1: цикл улучшения\n",
        "prompt = \"Какие существуют методы оценки знаний учащихся и в чём их различия?\"\n",
        "expected = (\n",
        "    \"Основные методы оценки знаний: формативная оценка (текущая обратная связь в процессе обучения), \"\n",
        "    \"суммативная оценка (итоговая проверка за период) и диагностическая оценка \"\n",
        "    \"(выявление пробелов перед началом обучения). Формативная направлена на корректировку процесса, \"\n",
        "    \"суммативная — на фиксацию результата, диагностическая — на определение стартового уровня.\"\n",
        ")\n",
        "\n",
        "history = iterative_improve(prompt, expected, max_iterations=3, target_relevance=0.85)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Пример: улучшение ответа о мотивации учащихся\n",
        "\n",
        "Попробуем тот же цикл на другом вопросе — о способах повышения мотивации."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Пример 2: другой вопрос\n",
        "prompt_2 = \"Как повысить учебную мотивацию школьников?\"\n",
        "expected_2 = (\n",
        "    \"Для повышения мотивации школьников эффективны: связь учебного материала с реальной жизнью, \"\n",
        "    \"предоставление выбора в заданиях, своевременная позитивная обратная связь, \"\n",
        "    \"использование игровых элементов (геймификация), постановка достижимых целей \"\n",
        "    \"и создание безопасной среды, где ошибки воспринимаются как часть обучения.\"\n",
        ")\n",
        "\n",
        "history_2 = iterative_improve(prompt_2, expected_2, max_iterations=3, target_relevance=0.80)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
